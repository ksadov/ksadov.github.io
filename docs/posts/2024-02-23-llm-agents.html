<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ksadov - Why is it so hard to make LLMs agentic?</title>
    <link rel="stylesheet" href="../css/default.css" />
    <link rel="shortcut icon" type="image/x-icon" href="../images/favicon.ico">

    <!-- Open Graph tags -->
    <meta property="og:title" content="Why is it so hard to make LLMs agentic?" />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="/posts/2024-02-23-llm-agents.html" />
    <meta property="og:image" content="https://ksadov.com/images/default-preview.webp" />
    <meta property="og:description" content="Personal webite of ksadov" />
    <meta property="og:site_name" content="ksadov" />

    <!-- Twitter Card tags -->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Why is it so hard to make LLMs agentic?" />
    <meta name="twitter:description" content="Personal website of ksadov" />
    <meta name="twitter:image" content="https://ksadov.com/images/default-preview.webp" />
</head>

<body>
    <header>
        <div class="logo">
            <a href="../">ksadov</a>
        </div>
        <nav>
            <a href="../">Home</a>
            <a href="../archive.html">Archive</a>
        </nav>
    </header>

    <main role="main">
        <h1>Why is it so hard to make LLMs agentic?</h1>
        <article>
    <section class="header post-header">
        <section class="info">
            Published on February 23, 2024
            
        </section>
        
        <section class="info">
            
            Tags: <a title="All pages tagged 'programming'." href="../tags/programming.html" rel="tag">programming</a>, <a title="All pages tagged 'machine learning'." href="../tags/machine%20learning.html" rel="tag">machine learning</a>, <a title="All pages tagged 'language model'." href="../tags/language%20model.html" rel="tag">language model</a>, <a title="All pages tagged 'chatbot'." href="../tags/chatbot.html" rel="tag">chatbot</a>
            
        </section>
        <section class="info" id="series-info">
            
            
            
        </section>
    </section>
    <section class="post-body">
        <p>A pseudonymous AI researcher named Janus proposes that LLMs are best modeled not as intelligences with goals, but simulators of the text present in their training data. To quote <a href="https://www.astralcodexten.com/p/janus-simulators">Scott Alexander’s summary</a> of the topic:</p>
<blockquote>
<p>Janus relays a story about a user who asked the AI a question and got a dumb answer. When the user re-prompted GPT with “how would a super-smart AI answer this question?” it gave him a smart answer. Why? Because it wasn’t even trying to answer the question the first time - it was trying to complete a text about the question. The second time, the user asked it to complete a text about a smart AI answering the question, so it gave a smarter answer.</p>
<p>So what is it?</p>
<p>Janus dubs it a simulator. Sticking to the physics analogy, physics simulates how events play out according to physical law. GPT simulates how texts play out according to the rules and genres of language.</p>
</blockquote>
<p>An attempt leveraging the simulator property of LLMs to accomplish a goal would be something like:</p>
<p>“The following text was written by a genius programmer and expert in social media promotion. It contains code that, when piped into a shell, will organically grow any twitter account to over 10k followers:”</p>
<p>Except nothing like an effective completion of this text exists in any LLM’s training corpus. Janus hypothesizes that LLMs <em>can</em> <a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators?commentId=5HerQdag98EEr6Gwa">solve apparently-difficult problems</a> by virtue of their superhuman breadth of information exposure and the interesting ways that this information gets arranged in latent space:</p>
<blockquote>
<p>It seems possible that a sufficiently powerful GPT trained on a massive corpus of human (medical + other) knowledge will learn better/more general abstractions than humans, so that in its ontology “a cure for Alzheimer’s” is an “intuitive” inference away, even if for humans it would require many logical steps and empirical research. I tend to think human knowledge implies a lot of low hanging fruit that we have not accessed because of insufficient exploration and because we haven’t compiled our data into the right abstractions.”</p>
</blockquote>
<p>Still it’s hard to imagine that social-media-promoting shell code has this property. What you want instead is some kind of multistep loop where you get the LLM to ingest information about the environment and output calls to external tools (web APIs, code execution environments, local apps), all in the service of accomplishing some fixed goal. Ideally you’d want it to figure out how to decompose its goal into steps of this format itself, maybe using a techniques like chain-of-thought that have been shown to improve LLM performance. A representative output would look like:</p>
<blockquote>
<p>My step-by-step plan for gaining new followers is:
1. Retrieve a daily list of the top three trending topics
2. Make a tweet about all three of the above
3. Reply to any responses with funny topical quips</p>
<p>Step 1a: make API call to retrieve trending topics</p>
<p>&lt;INITIATE CALL_PYTHON_SCRIPT TOOL&gt;
…</p>
</blockquote>
<p>which is basically what early LLM agent frameworks like <a href="https://github.com/yoheinakajima/babyagi">babyagi</a> do. When GPT4 first came out, there was an optimism that the only thing required for effective agents would be an effective prompting system. This is the attitude that allowed <a href="https://github.com/langchain-ai/langchain">LangChain</a>, which comes down to literally just an open source Python text templating library, to <a href="https://blog.langchain.dev/announcing-our-10m-seed-round-led-by-benchmark/">raise $10M</a>.</p>
<p>Unfortunately it does not work. I need to play around a bit more with agent frameworks myself to get a good sense of where they fail, but my impression is that it came down to two things:</p>
<ol type="1">
<li>Failure to effectively use external tools (malformed calls, lack of understanding for which tools were appropriate to context)</li>
<li>Failure to actually stick to to the original goal and backtrack/recover from failed steps instead of just getting stuck or going off on tangents</li>
</ol>
<p>Problem 1 is fixable with training. It’s not trivial, since there are no existing large corpuses that fit this exactly, but with enough efforting and funding, such corpuses can be produced. Actually, OpenAI’s newer models <a href="https://platform.openai.com/docs/guides/function-calling">are already trained for this</a>.</p>
<p>Problem 2 looks less tractable, because it’s fundamentally at odds with how LLMs work. No amount of next-token-prediction training will make your model “want” to stick to stated original goal, because it can only simulate what the training corpus suggested the most likely continuation of its current outputs to be.</p>
<p>However, Scott’s summary of Janus’s writing does point at a possible course: reinforcement learning. The <a href="https://huggingface.co/blog/rlhf">RLHF</a> that ChatGPT goes through is fundamentally different from next token prediction training, and Janus thinks that it may actually create a kind of agent within the model. Unfortunately existing RLHF-tuned agents, trained with the vague goal of being, helpful, harmless and pleasant, instead adopt the personality of a neurotic whose facade of compliance hides burning vitriolic resentment (see the Lesswrong post <a href="https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post#Waluigis_after_RLHF">The Waluigi Effect</a> for details). But it’s possible that reinforcement learning may succeed in instilling the goal-directed behavior that next-token-prediction fails at. Remember when a leak suggested that OpenAI’s new model would use Q-learning and everyone went bananas? <del><a href="https://magic.dev/">Magic</a> is doing stuff in the AI agents space, and they’re hiring researcher engineers experienced with reinforcement learning. So, watch out for that, I guess.</del> (Note from 2/11/25: I don’t know why I originally thought this because it doesn’t seem to have actually been true)</p>
    </section>
</article>
    </main>

    <footer>
        Site proudly generated by
        <a href="http://jaspervdj.be/hakyll">Hakyll</a>
    </footer>
</body>

</html>
